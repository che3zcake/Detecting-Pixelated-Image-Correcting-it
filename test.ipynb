{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from scipy.fftpack import dct\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def extract_patch_features(patch):\n",
    "    patch_float = patch.astype(np.float32) / 255.0\n",
    "\n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(patch, 100, 200)\n",
    "    edge_feature = np.sum(edges) / (patch.shape[0] * patch.shape[1])\n",
    "\n",
    "    # Block-like patterns using DCT\n",
    "    dct_features = dct(dct(patch_float, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "    dct_features = dct_features[:8, :8].flatten()\n",
    "\n",
    "    # Histogram of pixel intensities\n",
    "    hist = cv2.calcHist([patch], [0], None, [32], [0, 256])\n",
    "    hist_features = hist.flatten() / np.sum(hist)\n",
    "\n",
    "    # HOG for texture\n",
    "    hog_features = hog(patch, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2), feature_vector=True)\n",
    "\n",
    "    # Fast Fourier Transform for frequency analysis\n",
    "    f_transform = np.fft.fft2(patch_float)\n",
    "    f_transform = np.abs(np.fft.fftshift(f_transform))\n",
    "    f_features = f_transform[:8, :8].flatten()\n",
    "\n",
    "    return np.concatenate([\n",
    "        [edge_feature],\n",
    "        dct_features,\n",
    "        hist_features,\n",
    "        hog_features,\n",
    "        f_features\n",
    "    ])\n",
    "\n",
    "def process_patch(img, y, x, patch_size):\n",
    "    patch = img[y:y+patch_size, x:x+patch_size]\n",
    "    return extract_patch_features(patch)\n",
    "\n",
    "def extract_image_features(image_path, patch_size=128, stride=32):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    patch_features = []\n",
    "    patch_coords = [(y, x) for y in range(0, img.shape[0] - patch_size + 1, stride)\n",
    "                            for x in range(0, img.shape[1] - patch_size + 1, stride)]\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        patch_features = list(executor.map(lambda coords: process_patch(img, coords[0], coords[1], patch_size), patch_coords))\n",
    "\n",
    "    if not patch_features:\n",
    "        return None\n",
    "\n",
    "    patch_features = np.array(patch_features)\n",
    "\n",
    "    # Aggregate patch features\n",
    "    mean_features = np.mean(patch_features, axis=0)\n",
    "    max_features = np.max(patch_features, axis=0)\n",
    "    std_features = np.std(patch_features, axis=0)\n",
    "\n",
    "    # High pixelation ratio (using edge and DCT features)\n",
    "    edge_scores = patch_features[:, 0]\n",
    "    dct_scores = np.sum(patch_features[:, 1:65], axis=1)\n",
    "    high_edge_ratio = np.mean(edge_scores > np.median(edge_scores) * 1.5)\n",
    "    high_dct_ratio = np.mean(dct_scores > np.median(dct_scores) * 1.5)\n",
    "\n",
    "    return np.concatenate([\n",
    "        mean_features, max_features, std_features,\n",
    "        [high_edge_ratio, high_dct_ratio]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_dataset(data_dir):\n",
    "#     class_mapping = {'nonpixelated_img': 0, 'pixelated_img': 1}\n",
    "#     data = []\n",
    "#     labels = []\n",
    "\n",
    "#     for class_name in os.listdir(data_dir):\n",
    "#         class_dir = os.path.join(data_dir, class_name)\n",
    "#         if not os.path.isdir(class_dir):\n",
    "#             continue\n",
    "\n",
    "#         class_label = class_mapping.get(class_name)\n",
    "#         if class_label is None:\n",
    "#             print(f\"Skipping unknown class: {class_name}\")\n",
    "#             continue\n",
    "\n",
    "#         for img_file in tqdm(os.listdir(class_dir), desc=f\"Processing {class_name}\"):\n",
    "#             img_path = os.path.join(class_dir, img_file)\n",
    "#             features = extract_image_features(img_path)\n",
    "#             if features is not None:\n",
    "#                 data.append(features)\n",
    "#                 labels.append(class_label)\n",
    "\n",
    "#     return np.array(data), np.array(labels)\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     print(\"Building dataset...\")\n",
    "#     X, y = build_dataset(r'C:\\Users\\KIIT\\Desktop\\Projects\\dataset')\n",
    "\n",
    "#     if len(X) == 0:\n",
    "#         print(\"No valid data extracted. Please check your dataset.\")\n",
    "#     else:\n",
    "#         print(f\"Dataset built: {len(X)} samples, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset using joblib\n",
    "file_path = r'C:\\Users\\KIIT\\Desktop\\Projects\\model\\dataset.pkl'\n",
    "X, y = joblib.load(file_path)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Pixelated       0.97      0.99      0.98       715\n",
      "    Pixelated       0.99      0.97      0.98       956\n",
      "\n",
      "     accuracy                           0.98      1671\n",
      "    macro avg       0.98      0.98      0.98      1671\n",
      " weighted avg       0.98      0.98      0.98      1671\n",
      "\n",
      "Confusion Matrix:\n",
      "[[705  10]\n",
      " [ 25 931]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a basic model\n",
    "clf = RandomForestClassifier(n_estimators = 200, min_samples_split = 2, min_samples_leaf = 1, max_depth = 30, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Pixelated', 'Pixelated']))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.99\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Pixelated       0.98      0.99      0.99       715\n",
      "    Pixelated       0.99      0.99      0.99       956\n",
      "\n",
      "     accuracy                           0.99      1671\n",
      "    macro avg       0.99      0.99      0.99      1671\n",
      " weighted avg       0.99      0.99      0.99      1671\n",
      "\n",
      "Confusion Matrix:\n",
      "[[709   6]\n",
      " [ 13 943]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Create and fit the HistGradientBoostingClassifier\n",
    "hclf = HistGradientBoostingClassifier(random_state=42)\n",
    "hclf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "score = hclf.score(X_test, y_test)\n",
    "print(f'Model accuracy: {score:.2f}')\n",
    "\n",
    "y_pred = hclf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Pixelated', 'Pixelated']))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.99\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       715\n",
      "           1       0.99      0.99      0.99       956\n",
      "\n",
      "    accuracy                           0.99      1671\n",
      "   macro avg       0.99      0.99      0.99      1671\n",
      "weighted avg       0.99      0.99      0.99      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgbm = xgb.XGBClassifier(n_estimators= 200, max_depth= 4, learning_rate= 0.17428234843001944, subsample= 0.6129965942712094, colsample_bytree= 0.8184848884905318, gamma= 0.05718479848470478)\n",
    "xgbm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "score = xgbm.score(X_test, y_test)\n",
    "print(f'Model accuracy: {score:.2f}')\n",
    "\n",
    "y_pred = xgbm.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacking Classifier: 0.99\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       715\n",
      "           1       0.99      0.99      0.99       956\n",
      "\n",
      "    accuracy                           0.99      1671\n",
      "   macro avg       0.99      0.99      0.99      1671\n",
      "weighted avg       0.99      0.99      0.99      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Define the Stacking Classifier\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators = 200, min_samples_split = 2, min_samples_leaf = 1, max_depth = 30, random_state=42)),\n",
    "    ('xgb', xgb.XGBClassifier(n_estimators= 200, max_depth= 4, learning_rate= 0.17428234843001944, subsample= 0.6129965942712094, colsample_bytree= 0.8184848884905318, gamma= 0.05718479848470478)),\n",
    "    ('hclf', HistGradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=MLPClassifier(random_state=42),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = stack.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Stacking Classifier: {accuracy:.2f}')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    # Extract features from the new imagenp.\n",
    "    features = extract_image_features(image_path)\n",
    "    if features is None:\n",
    "        print(\"Failed to extract features from the image.\")\n",
    "        return None\n",
    "    \n",
    "    # Reshape features to match the expected input format of the model\n",
    "    features = features.reshape(1, -1)\n",
    "    \n",
    "    # Predict using the loaded model\n",
    "    prediction = clf.predict(features)\n",
    "    probability = clf.predict_proba(features)[0][1]\n",
    "\n",
    "    hprediction = hclf.predict(features)\n",
    "    hprobability = hclf.predict_proba(features)[0][1]\n",
    "\n",
    "    xprediction = xgbm.predict(features)\n",
    "    xprobability = xgbm.predict_proba(features)[0][1]\n",
    "\n",
    "    sprediction = stack.predict(features)\n",
    "    sprobability = stack.predict_proba(features)[0][1]\n",
    "    \n",
    "    \n",
    "    return prediction[0], probability, hprediction[0], hprobability, xprediction[0], xprobability, sprediction[0], sprobability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLF Prediction: Not Pixelated\n",
      "HCLF Prediction: Not Pixelated\n",
      "XGB Prediction: Not Pixelated\n",
      "Stack Prediction: Not Pixelated\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    new_image_path = r\"C:\\Users\\KIIT\\Desktop\\Projects\\backiee-123754.jpg\"\n",
    "    \n",
    "    prediction, probability, hprediction, hprobability, xprediction, xprobability, sprediction, sprobability = predict_image(new_image_path)\n",
    "    \n",
    "    print(f\"CLF Prediction: {'Pixelated' if prediction == 1 else 'Not Pixelated'}\")\n",
    "    print(f\"HCLF Prediction: {'Pixelated' if hprediction == 1 else 'Not Pixelated'}\")\n",
    "    print(f\"XGB Prediction: {'Pixelated' if xprediction == 1 else 'Not Pixelated'}\")\n",
    "    print(f\"Stack Prediction: {'Pixelated' if sprediction == 1 else 'Not Pixelated'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved.\n"
     ]
    }
   ],
   "source": [
    "joblib.dump((X, y), os.path.join(r'C:\\Users\\KIIT\\Desktop\\Projects\\model', \"dataset.pkl\"))\n",
    "print(\"Dataset saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(stack, os.path.join(r'C:\\Users\\KIIT\\Desktop\\Projects\\model', 'pixelation_model.pkl'))\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Specify the path to the saved model\n",
    "model_path = os.path.join(r'C:\\Users\\KIIT\\Desktop\\Projects\\model', 'pixelation_model.pkl')\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = joblib.load(model_path)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_image_features(r'C:\\Users\\KIIT\\Desktop\\Projects\\WhatsApp Image 2024-06-05 at 10.58.04_4627b58c.jpg')\n",
    "features = features.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the loaded model\n",
    "predictions = loaded_model.predict(features)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters found:  {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.2, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "Best accuracy found:  0.9844304929711756\n",
      "Test set accuracy: 98.76%\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [3, 4, 5, 6],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "#     'subsample': [0.6, 0.8, 1.0],\n",
    "#     'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#     'gamma': [0, 0.1, 0.2, 0.3]\n",
    "# }\n",
    "\n",
    "# # Set up the grid search\n",
    "# random_search = RandomizedSearchCV(estimator=xgbm, param_distributions=param_grid, n_iter=50, cv=3, scoring='accuracy', verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "# # Fit the model\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and the best score\n",
    "# best_params = random_search.best_params_\n",
    "# best_score = random_search.best_score_\n",
    "\n",
    "# print(\"Best parameters found: \", best_params)\n",
    "# print(\"Best accuracy found: \", best_score)\n",
    "\n",
    "# # Train the model with the best parameters\n",
    "# best_xgb = random_search.best_estimator_\n",
    "# best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Test set accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "\n",
    "# # Define the objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "#         'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "#         'gamma': trial.suggest_uniform('gamma', 0, 0.3)\n",
    "#     }\n",
    "    \n",
    "#     xgb_clf = xgb.XGBClassifier(**param, use_label_encoder=False, eval_metric='logloss')\n",
    "#     xgb_clf.fit(X_train, y_train)\n",
    "    \n",
    "#     preds = xgb_clf.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Create a study object and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# best_params = study.best_params\n",
    "# best_score = study.best_value\n",
    "\n",
    "# print(\"Best parameters found: \", best_params)\n",
    "# print(\"Best accuracy found: \", best_score)\n",
    "\n",
    "# # Train the model with the best parameters\n",
    "# best_xgb = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
    "# best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Test set accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_iter': [100, 200, 300],\n",
    "#     'max_leaf_nodes': [31, 63, 127],\n",
    "#     'max_depth': [None, 3, 5, 7],\n",
    "#     'min_samples_leaf': [20, 50, 100],\n",
    "#     'l2_regularization': [0, 0.1, 1],\n",
    "#     'max_bins': [255, 512]\n",
    "# }\n",
    "\n",
    "\n",
    "# # Initialize Grid Search\n",
    "# grid_search = GridSearchCV(estimator=hclf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# # Fit Grid Search\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best cross-validation accuracy: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)aa\n",
    "# print(\"Test set accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
